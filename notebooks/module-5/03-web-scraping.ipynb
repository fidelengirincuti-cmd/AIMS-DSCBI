{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422cffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Web scraping demo: Books to Scrape\n",
    "# https://books.toscrape.com/\n",
    "# - Scrapes title, price, rating, availability, and product URL\n",
    "# - Follows pagination (Next →)\n",
    "# - Optional: scrape a specific category\n",
    "\n",
    "# %%\n",
    "import time, re\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "BASE = \"https://books.toscrape.com/\"\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                      \"(KHTML, like Gecko) Chrome Safari\"\n",
    "    })\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.4,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    return s\n",
    "\n",
    "def clean_price(txt):\n",
    "    # '£51.77' -> 51.77\n",
    "    if not txt: return None\n",
    "    m = re.search(r\"(\\d+(?:\\.\\d+)?)\", txt.replace(\",\", \"\"))\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def get_soup(session, url, sleep=0.2, timeout=15):\n",
    "    resp = session.get(url, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    time.sleep(sleep)  # be polite\n",
    "    return BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "def parse_book_card(card, page_url):\n",
    "    # Each book is inside <article class=\"product_pod\">\n",
    "    title_el = card.select_one(\"h3 a\")\n",
    "    title = title_el.get(\"title\") if title_el else None\n",
    "    rel_link = title_el.get(\"href\") if title_el else None\n",
    "    product_url = urljoin(page_url, rel_link) if rel_link else None\n",
    "\n",
    "    price = clean_price(card.select_one(\".price_color\").get_text(strip=True) if card.select_one(\".price_color\") else None)\n",
    "\n",
    "    # Rating is in class, e.g., <p class=\"star-rating Three\">\n",
    "    rating_el = card.select_one(\".star-rating\")\n",
    "    rating = None\n",
    "    if rating_el:\n",
    "        classes = rating_el.get(\"class\", [])\n",
    "        # classes like [\"star-rating\", \"Three\"]\n",
    "        for c in classes:\n",
    "            if c in {\"One\",\"Two\",\"Three\",\"Four\",\"Five\"}:\n",
    "                rating = c\n",
    "\n",
    "    # availability appears on product page; on list page it may not be present\n",
    "    availability = card.select_one(\".availability\")\n",
    "    availability = availability.get_text(strip=True) if availability else None\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"price\": price,\n",
    "        \"rating\": rating,\n",
    "        \"availability\": availability,\n",
    "        \"product_url\": product_url,\n",
    "        \"page_url\": page_url,\n",
    "    }\n",
    "\n",
    "def iter_pages(start_url, session, limit_pages=None):\n",
    "    \"\"\"Yield soup & url for each page, following 'next' links.\"\"\"\n",
    "    url = start_url\n",
    "    pages = 0\n",
    "    while url:\n",
    "        soup = get_soup(session, url)\n",
    "        yield soup, url\n",
    "        pages += 1\n",
    "        if limit_pages and pages >= limit_pages:\n",
    "            break\n",
    "        next_link = soup.select_one(\"li.next a\")\n",
    "        url = urljoin(url, next_link.get(\"href\")) if next_link else None\n",
    "\n",
    "def scrape_books(category_url=None, limit_pages=None):\n",
    "    \"\"\"\n",
    "    Scrape all books from the main listing or a specific category.\n",
    "    - category_url example: 'https://books.toscrape.com/catalogue/category/books/travel_2/index.html'\n",
    "    \"\"\"\n",
    "    session = make_session()\n",
    "    start = category_url or urljoin(BASE, \"catalogue/page-1.html\")  # main listing\n",
    "    rows = []\n",
    "\n",
    "    for soup, page_url in iter_pages(start, session, limit_pages=limit_pages):\n",
    "        cards = soup.select(\"article.product_pod\")\n",
    "        for c in cards:\n",
    "            rows.append(parse_book_card(c, page_url))\n",
    "        print(f\"[info] {page_url} -> +{len(cards)} items (total {len(rows)})\")\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# %% Run: scrape first 2 pages of all books (quick classroom run)\n",
    "df_demo = scrape_books(limit_pages=2)\n",
    "df_demo.head(), df_demo.shape\n",
    "\n",
    "# %% Optional: scrape full site (all pages). Comment back in if you want the full run.\n",
    "# df_all = scrape_books(limit_pages=None)\n",
    "# df_all.to_csv(\"books_all.csv\", index=False)\n",
    "# print(\"Saved books_all.csv with\", len(df_all), \"rows\")\n",
    "\n",
    "# %% Optional: pick a category\n",
    "# How to find a category: browse homepage → click a category → copy its URL\n",
    "# Example (Travel):\n",
    "# cat_url = \"https://books.toscrape.com/catalogue/category/books/travel_2/index.html\"\n",
    "# df_travel = scrape_books(category_url=cat_url, limit_pages=None)\n",
    "# df_travel.to_csv(\"books_travel.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
