{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0743320",
   "metadata": {},
   "source": [
    "\n",
    "# Scikit-learn Processes: Data Prep â†’ Modeling â†’ Evaluation (1â€“2 hours)\n",
    "\n",
    "**Goal:** Hands-on tour of the *end-to-end* scikit-learn workflow using a small, **census-like** synthetic dataset with both numeric and categorical features.\n",
    "\n",
    "**What you'll learn**\n",
    "- Create a toy dataset that resembles census microdata\n",
    "- Train/test split & baselines\n",
    "- Build preprocessing pipelines: imputation, scaling, one-hot encoding\n",
    "- Fit models (logistic regression, SGD with different **loss** functions, random forest)\n",
    "- Evaluate with multiple metrics (accuracy, F1, ROC AUC, confusion matrix)\n",
    "- Cross-validation & model selection with **GridSearchCV**\n",
    "- Save & load trained pipelines\n",
    "- (Optional) short regression mini-example, including choices of regression losses\n",
    "\n",
    "> ðŸ’¡ You can later swap in your **real census** CSV and keep the same pipeline structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Ridge, SGDRegressor, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, RocCurveDisplay,\n",
    "    ConfusionMatrixDisplay, classification_report,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "pd.set_option(\"display.max_columns\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da517b",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Build a small census-like dataset\n",
    "\n",
    "We'll simulate a binary **employment_status** target:\n",
    "- `1` = employed, `0` = not employed\n",
    "\n",
    "**Features**\n",
    "- Numeric: `age`, `household_size`\n",
    "- Categorical: `sex` (`M`/`F`), `education` (`none`, `primary`, `secondary`, `tertiary`), `region` (`north`, `center`, `south`)\n",
    "- Binary service access: `electricity` (`yes`/`no`)\n",
    "\n",
    "We'll also inject a few missing values to practice imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db764d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 900\n",
    "\n",
    "age = np.random.randint(15, 70, size=n)\n",
    "household_size = np.clip(np.round(np.random.normal(4.5, 1.8, size=n)), 1, 12).astype(int)\n",
    "\n",
    "sex = np.random.choice([\"M\", \"F\"], size=n, p=[0.48, 0.52])\n",
    "education = np.random.choice([\"none\", \"primary\", \"secondary\", \"tertiary\"],\n",
    "                             size=n, p=[0.18, 0.38, 0.34, 0.10])\n",
    "region = np.random.choice([\"north\", \"center\", \"south\"], size=n, p=[0.2, 0.45, 0.35])\n",
    "electricity = np.random.choice([\"yes\", \"no\"], size=n, p=[0.72, 0.28])\n",
    "\n",
    "# Latent propensity for employment (log-odds)\n",
    "logit = (\n",
    "    -3.0\n",
    "    + 0.06 * age\n",
    "    - 0.12 * (household_size - 4)\n",
    "    + np.select(\n",
    "        [\n",
    "            education == \"none\",\n",
    "            education == \"primary\",\n",
    "            education == \"secondary\",\n",
    "            education == \"tertiary\",\n",
    "        ],\n",
    "        [-0.8, -0.2, 0.4, 1.0], default=0.0\n",
    "    )\n",
    "    + np.where(sex == \"M\", 0.2, 0.0)\n",
    "    + np.where(electricity == \"yes\", 0.4, -0.1)\n",
    "    + np.select([region==\"north\", region==\"center\", region==\"south\"], [0.1, 0.0, -0.05])\n",
    ")\n",
    "\n",
    "# Convert to probability with logistic\n",
    "p = 1 / (1 + np.exp(-logit))\n",
    "employed = (np.random.rand(n) < p).astype(int)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"age\": age,\n",
    "    \"household_size\": household_size,\n",
    "    \"sex\": sex,\n",
    "    \"education\": education,\n",
    "    \"region\": region,\n",
    "    \"electricity\": electricity,\n",
    "    \"employed\": employed\n",
    "})\n",
    "\n",
    "# Introduce some missingness\n",
    "for col in [\"age\", \"household_size\", \"sex\", \"education\", \"electricity\"]:\n",
    "    mask = np.random.rand(n) < 0.05\n",
    "    df.loc[mask, col] = np.nan\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01149db6",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Train/test split & baseline\n",
    "\n",
    "We keep a **test set** for honest evaluation.\n",
    "We'll also compute a majority-class baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "X = df.drop(columns=[\"employed\"])\n",
    "y = df[\"employed\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "baseline = DummyClassifier(strategy=\"most_frequent\", random_state=RANDOM_STATE)\n",
    "baseline.fit(X_train, y_train)\n",
    "y_base = baseline.predict(X_test)\n",
    "print(\"Baseline accuracy:\", round(accuracy_score(y_test, y_base), 3))\n",
    "print(\"Baseline F1:\", round(f1_score(y_test, y_base, zero_division=0), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab36c7e",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Preprocessing pipelines\n",
    "\n",
    "- **Numeric**: median imputation â†’ scaling (we'll try both `StandardScaler` and `MinMaxScaler` later)\n",
    "- **Categorical**: most-frequent imputation â†’ one-hot encoding\n",
    "\n",
    "We use a `ColumnTransformer` to apply the right transforms to the right columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = [\"age\", \"household_size\"]\n",
    "cat_cols = [\"sex\", \"education\", \"region\", \"electricity\"]\n",
    "\n",
    "numeric_standard = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "numeric_minmax = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "categorical = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess_standard = ColumnTransformer([\n",
    "    (\"num\", numeric_standard, num_cols),\n",
    "    (\"cat\", categorical, cat_cols)\n",
    "])\n",
    "\n",
    "preprocess_minmax = ColumnTransformer([\n",
    "    (\"num\", numeric_minmax, num_cols),\n",
    "    (\"cat\", categorical, cat_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e4c6b",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Fit first model: Logistic Regression (log-loss)\n",
    "\n",
    "Logistic Regression implicitly optimizes **logistic loss** (a.k.a. log loss).  \n",
    "We'll measure Accuracy, F1, and AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28708c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logreg_pipe = Pipeline([\n",
    "    (\"prep\", preprocess_standard),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "logreg_pipe.fit(X_train, y_train)\n",
    "y_pred = logreg_pipe.predict(X_test)\n",
    "y_prob = logreg_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"LogReg accuracy:\", round(accuracy_score(y_test, y_pred), 3))\n",
    "print(\"LogReg F1:\", round(f1_score(y_test, y_pred), 3))\n",
    "print(\"LogReg ROC AUC:\", round(roc_auc_score(y_test, y_prob), 3))\n",
    "\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(logreg_pipe, X_test, y_test)\n",
    "plt.title(\"Confusion Matrix: Logistic Regression\")\n",
    "plt.show()\n",
    "\n",
    "RocCurveDisplay.from_estimator(logreg_pipe, X_test, y_test)\n",
    "plt.title(\"ROC Curve: Logistic Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c85b5",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Exploring **loss functions** with `SGDClassifier`\n",
    "\n",
    "`SGDClassifier` supports multiple loss functions:\n",
    "- `\"log_loss\"` â†’ logistic regression (probabilistic, good for AUC/LogLoss)\n",
    "- `\"hinge\"` â†’ linear SVM (max-margin, works well with scaled features)\n",
    "- `\"modified_huber\"` â†’ robust hinge-like loss with probabilities\n",
    "\n",
    "We'll compare via cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sgd_losses = [\"log_loss\", \"hinge\", \"modified_huber\"]\n",
    "cv_results = {}\n",
    "for loss in sgd_losses:\n",
    "    sgd_pipe = Pipeline([\n",
    "        (\"prep\", preprocess_standard),\n",
    "        (\"clf\", SGDClassifier(loss=loss, random_state=RANDOM_STATE, max_iter=2000, tol=1e-3))\n",
    "    ])\n",
    "    # For hinge we evaluate accuracy; for log_loss/modified_huber we can also do ROC AUC\n",
    "    scores = cross_val_score(sgd_pipe, X, y, cv=5, scoring=\"accuracy\")\n",
    "    cv_results[loss] = scores\n",
    "\n",
    "for loss, scores in cv_results.items():\n",
    "    print(f\"Loss={loss:>14} | CV Accuracy: {scores.mean():.3f} Â± {scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe438923",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Model selection with `GridSearchCV`\n",
    "\n",
    "We'll search:\n",
    "- Preprocessing scaler (`Standard` vs `MinMax`)\n",
    "- Classifier type (Logistic Regression vs SGD)\n",
    "- SGD **loss** function & regularization `alpha`\n",
    "\n",
    "> Note: This is a small space so it runs fast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocess_standard),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"prep\": [preprocess_standard, preprocess_minmax],\n",
    "        \"clf\": [LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)],\n",
    "        \"clf__C\": [0.5, 1.0, 2.0]\n",
    "    },\n",
    "    {\n",
    "        \"prep\": [preprocess_standard, preprocess_minmax],\n",
    "        \"clf\": [SGDClassifier(random_state=RANDOM_STATE, max_iter=2000, tol=1e-3)],\n",
    "        \"clf__loss\": [\"log_loss\", \"hinge\", \"modified_huber\"],\n",
    "        \"clf__alpha\": [1e-4, 1e-3, 1e-2]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe, param_grid=param_grid, cv=5, scoring=\"f1\", n_jobs=None, refit=True\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV F1:\", round(grid.best_score_, 3))\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "yhat = best_model.predict(X_test)\n",
    "yhat_prob = None\n",
    "try:\n",
    "    yhat_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"\\nTest metrics with best model:\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, yhat), 3))\n",
    "print(\"F1:\", round(f1_score(y_test, yhat), 3))\n",
    "if yhat_prob is not None:\n",
    "    print(\"ROC AUC:\", round(roc_auc_score(y_test, yhat_prob), 3))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test)\n",
    "plt.title(\"Confusion Matrix: Best Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e1ff92",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Tree-based model (no scaling needed): RandomForest\n",
    "\n",
    "Good to contrast with linear models. Trees are scale-invariant and often strong baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"prep\", ColumnTransformer([\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols)\n",
    "    ])),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "pred_rf = rf_pipe.predict(X_test)\n",
    "print(\"RandomForest accuracy:\", round(accuracy_score(y_test, pred_rf), 3))\n",
    "print(\"RandomForest F1:\", round(f1_score(y_test, pred_rf), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a759a",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Save & load the trained pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8076167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = \"/mnt/data/employment_best_pipeline.joblib\"\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "loaded = joblib.load(model_path)\n",
    "sample = X_test.iloc[[0]]\n",
    "print(\"Sample row:\\n\", sample)\n",
    "print(\"\\nLoaded model prediction:\", loaded.predict(sample)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e81b094",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## (Optional) Mini Regression Example: choosing losses\n",
    "\n",
    "We'll predict a synthetic **wealth_index** and compare losses:\n",
    "- `LinearRegression` (optimizes **MSE**)\n",
    "- `Ridge` (MSE + L2 penalty)\n",
    "- `SGDRegressor` with `loss=\"squared_error\"` (MSE) vs `loss=\"huber\"` (robust to outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd52ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a numeric 'wealth_index' linked to features\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "wealth = (\n",
    "    0.03 * df[\"age\"].fillna(df[\"age\"].median()).to_numpy()\n",
    "    + 0.5 * (df[\"education\"] == \"secondary\").fillna(False).to_numpy().astype(float)\n",
    "    + 1.0 * (df[\"education\"] == \"tertiary\").fillna(False).to_numpy().astype(float)\n",
    "    + 0.6 * (df[\"electricity\"] == \"yes\").fillna(False).to_numpy().astype(float)\n",
    "    - 0.05 * df[\"household_size\"].fillna(df[\"household_size\"].median()).to_numpy()\n",
    "    + rng.normal(0, 0.5, size=len(df))\n",
    ")\n",
    "\n",
    "df_reg = df.copy()\n",
    "df_reg[\"wealth_index\"] = wealth\n",
    "\n",
    "Xr = df_reg.drop(columns=[\"wealth_index\", \"employed\"])\n",
    "yr = df_reg[\"wealth_index\"]\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    Xr, yr, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "reg_prep = ColumnTransformer([\n",
    "    (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "# LinearRegression\n",
    "reg_lr = Pipeline([(\"prep\", reg_prep), (\"reg\", LinearRegression())])\n",
    "reg_lr.fit(Xr_train, yr_train)\n",
    "pred_lr = reg_lr.predict(Xr_test)\n",
    "\n",
    "# Ridge\n",
    "reg_ridge = Pipeline([(\"prep\", reg_prep), (\"reg\", Ridge(alpha=1.0, random_state=RANDOM_STATE))])\n",
    "reg_ridge.fit(Xr_train, yr_train)\n",
    "pred_ridge = reg_ridge.predict(Xr_test)\n",
    "\n",
    "# SGDRegressor (MSE)\n",
    "reg_sgd_mse = Pipeline([\n",
    "    (\"prep\", reg_prep),\n",
    "    (\"reg\", SGDRegressor(loss=\"squared_error\", random_state=RANDOM_STATE, max_iter=2000, tol=1e-3))\n",
    "])\n",
    "reg_sgd_mse.fit(Xr_train, yr_train)\n",
    "pred_sgd_mse = reg_sgd_mse.predict(Xr_test)\n",
    "\n",
    "# SGDRegressor (Huber)\n",
    "reg_sgd_huber = Pipeline([\n",
    "    (\"prep\", reg_prep),\n",
    "    (\"reg\", SGDRegressor(loss=\"huber\", random_state=RANDOM_STATE, max_iter=2000, tol=1e-3))\n",
    "])\n",
    "reg_sgd_huber.fit(Xr_train, yr_train)\n",
    "pred_sgd_huber = reg_sgd_huber.predict(Xr_test)\n",
    "\n",
    "def eval_reg(y_true, y_hat, name):\n",
    "    mae = mean_absolute_error(y_true, y_hat)\n",
    "    rmse = mean_squared_error(y_true, y_hat, squared=False)\n",
    "    r2 = r2_score(y_true, y_hat)\n",
    "    print(f\"{name:>15} | MAE={mae:.3f} | RMSE={rmse:.3f} | R^2={r2:.3f}\")\n",
    "\n",
    "print(\"Regression comparison:\")\n",
    "eval_reg(yr_test, pred_lr, \"LinearRegression\")\n",
    "eval_reg(yr_test, pred_ridge, \"Ridge\")\n",
    "eval_reg(yr_test, pred_sgd_mse, \"SGD (MSE)\")\n",
    "eval_reg(yr_test, pred_sgd_huber, \"SGD (Huber)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e61ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple true vs predicted plot for one model\n",
    "plt.figure()\n",
    "plt.scatter(yr_test, pred_ridge)\n",
    "plt.xlabel(\"True wealth_index\")\n",
    "plt.ylabel(\"Predicted wealth_index\")\n",
    "plt.title(\"Regression: True vs Predicted (Ridge)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93339f",
   "metadata": {},
   "source": [
    "\n",
    "## Cheat-sheet: choosing losses & scalers\n",
    "\n",
    "- **Classification**\n",
    "  - **`log_loss`** (Logistic Regression/SGD): probabilistic outputs; supports AUC/LogLoss metrics; great default.\n",
    "  - **`hinge`** (Linear SVM via SGD): max-margin; works well with **scaled** features; no probabilities by default.\n",
    "  - **`modified_huber`**: robust hinge-like; gives usable probabilities.\n",
    "- **Regression**\n",
    "  - **MSE / squared_error**: default; sensitive to outliers; pairs well with Ridge/Lasso.\n",
    "  - **Huber**: robust to outliers; good default when you expect noise.\n",
    "- **Scaling**\n",
    "  - **StandardScaler**: centers to 0 mean, unit variance; good for linear models/SGD/SVM.\n",
    "  - **MinMaxScaler**: squashes to [0, 1]; useful for bounded features or when algorithms assume a range.\n",
    "\n",
    "**Practical tips**\n",
    "- Always **impute** missing values inside the pipeline.\n",
    "- Use **`ColumnTransformer`** to keep numeric vs categorical paths clean.\n",
    "- Start with a **simple baseline** and **cross-validation**.\n",
    "- Compare a **linear** model with a **tree** model.\n",
    "- Save the **whole pipeline** (preprocessing + model).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
